# Demo 3: CSV Data Transformation Pipeline

## Overview

Demonstrates a complete ETL (Extract, Transform, Load) pipeline using multiple chained Transform streams for CSV data processing.

## Structure

```
demo3/
â”œâ”€â”€ index.js                  # Main entry point
â”œâ”€â”€ CSVParser.js             # CSV to object parser
â”œâ”€â”€ DataValidator.js         # Validation and filtering
â”œâ”€â”€ DataEnricher.js          # Data enrichment
â”œâ”€â”€ StatisticsCollector.js   # Real-time stats
â”œâ”€â”€ formatters.js            # JSON & CSV output formatters
â”œâ”€â”€ csvGenerator.js          # Sample CSV generator
â”œâ”€â”€ csvProcessor.js          # Main pipeline logic
â””â”€â”€ README.md               # This file
```

## Modules

### CSVParser.js

Transform stream that converts CSV to objects:

- Extracts headers from first line
- Parses each row into JavaScript object
- Handles object mode streams

### DataValidator.js

Validates data against rules:

- Configurable validation rules
- Filters invalid records
- Tracks validation statistics

### DataEnricher.js

Adds computed fields:

- Age group calculation
- Email domain extraction
- Custom enrichment functions
- Extensible enrichment pipeline

### StatisticsCollector.js

Collects real-time statistics:

- Counts by category
- Calculates averages
- Pass-through stream (doesn't modify data)

### formatters.js

Output formatting streams:

- `JSONFormatter` - Creates JSON arrays
- `CSVFormatter` - Creates CSV files
- Both work in object mode

### csvProcessor.js

Main pipeline orchestration:

- Chains all transforms
- Splits output to multiple destinations
- Error handling
- Returns processing results

## Usage

```bash
# Run the demo
node index.js
```

## Pipeline Flow

```
Input CSV File
    â†“
CSVParser (CSV â†’ Objects)
    â†“
DataValidator (Filter invalid)
    â†“
DataEnricher (Add computed fields)
    â†“
StatisticsCollector (Collect stats)
    â†“
Splitter (Duplicate stream)
    â†“           â†“
JSONFormatter  CSVFormatter
    â†“           â†“
output.json   output.csv
```

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          CSV TRANSFORMATION PIPELINE                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

INPUT FILE: employees.csv
     â”‚
     â”‚ (Raw CSV text)
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  fs.createReadStreamâ”‚  ðŸ“„ Read file chunks
â”‚   (Readable Stream) â”‚  
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â”‚ Chunks: "name,age,department\nJohn,30,IT\n..."
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    CSVParser        â”‚  ðŸ” Parse CSV â†’ Objects
â”‚ (Transform Stream)  â”‚  Input:  "John,30,IT\n"
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  Output: {name:"John", age:"30", dept:"IT"}
           â”‚
           â”‚ Objects: {name, age, department, ...}
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   DataValidator     â”‚  âœ… Validate data
â”‚ (Transform Stream)  â”‚  - Check required fields
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  - Validate data types
           â”‚              - Filter invalid records
           â”‚ Valid objects only
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   DataEnricher      â”‚  âœ¨ Enrich data
â”‚ (Transform Stream)  â”‚  - Add fullName
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  - Add ageGroup
           â”‚              - Add timestamp
           â”‚ Enriched objects
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ StatisticsCollector â”‚  ðŸ“Š Collect stats
â”‚ (Transform Stream)  â”‚  - Count records
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  - Track departments
           â”‚              - Calculate averages
           â”‚ (Pass through + collect stats)
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Splitter        â”‚  ðŸ”€ Split data flow
â”‚  (PassThrough)      â”‚  Data goes to BOTH branches
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚                          â”‚                          â”‚
           â”‚ Branch 1                 â”‚ Branch 2                 â”‚
           â–¼                          â–¼                          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚   PassThrough       â”‚    â”‚   PassThrough       â”‚              â”‚
â”‚  (objectMode)       â”‚    â”‚  (objectMode)       â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
           â”‚                          â”‚                          â”‚
           â”‚ Objects                  â”‚ Objects                  â”‚
           â–¼                          â–¼                          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚   JSONFormatter     â”‚    â”‚   CSVFormatter      â”‚              â”‚
â”‚ (Transform Stream)  â”‚    â”‚ (Transform Stream)  â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
           â”‚                          â”‚                          â”‚
           â”‚ JSON strings             â”‚ CSV strings              â”‚
           â”‚ "{"name":"John",...}\n"  â”‚ "John,30,IT,...\n"      â”‚
           â–¼                          â–¼                          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚fs.createWriteStream â”‚    â”‚fs.createWriteStream â”‚              â”‚
â”‚ (Writable Stream)   â”‚    â”‚ (Writable Stream)   â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
           â”‚                          â”‚                          â”‚
           â–¼                          â–¼                          â”‚
    OUTPUT FILE:             OUTPUT FILE:                        â”‚
  employees.json          employees.csv                          â”‚
                                                                  â”‚
                                                                  â”‚
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
                          â”‚   Statistics        â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚   (Collected data)  â”‚  getStats()
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          {
                            totalRecords: 100,
                            departments: {...},
                            averageAge: 35.5
                          }
```



```
Step-by-step transformation of ONE record:

1ï¸âƒ£ Raw CSV Input:
   "John Doe,30,IT,john@example.com,60000\n"
   
2ï¸âƒ£ After CSVParser:
   {
     name: "John Doe",
     age: "30",
     department: "IT",
     email: "john@example.com",
     salary: "60000"
   }
   
3ï¸âƒ£ After DataValidator:
   {
     name: "John Doe",
     age: 30,                    // â† Converted to number
     department: "IT",
     email: "john@example.com",
     salary: 60000               // â† Converted to number
   }
   
4ï¸âƒ£ After DataEnricher:
   {
     name: "John Doe",
     age: 30,
     department: "IT",
     email: "john@example.com",
     salary: 60000,
     fullName: "John Doe",       // â† Added
     ageGroup: "adult",          // â† Added
     processedAt: "2024-11-15"   // â† Added
   }
   
5ï¸âƒ£ After StatisticsCollector:
   (Same object, but stats collected in background)
   Stats: { totalRecords: 1, departments: {IT: 1}, ... }
   
6ï¸âƒ£ At Splitter:
   Object flows to BOTH branches simultaneously
   
7ï¸âƒ£ Branch 1 - JSONFormatter:
   '{"name":"John Doe","age":30,"department":"IT",...}\n'
   
8ï¸âƒ£ Branch 2 - CSVFormatter:
   'John Doe,30,IT,john@example.com,60000,John Doe,adult,2024-11-15\n'
   
9ï¸âƒ£ Final Output:
   - employees.json: Contains JSON formatted data
   - employees.csv: Contains enriched CSV data

```


## Customization

### Custom Validation Rules

```javascript
const validator = new DataValidator({
  validationRules: (row) => {
    return row.age > 21 && row.city !== "";
  },
});
```

### Custom Enrichment

```javascript
const enricher = new DataEnricher({
  enrichmentFunctions: [
    (row) => ({ ...row, fullName: row.name.toUpperCase() }),
    (row) => ({ ...row, senior: row.age >= 65 }),
  ],
});
```

## Learning Points

- âœ… Custom Transform streams in object mode
- âœ… Chaining multiple transforms with `pipeline()`
- âœ… Stream splitting for multiple outputs
- âœ… Pass-through streams for statistics
- âœ… Real-time data processing
- âœ… Memory-efficient ETL operations
- âœ… Modular stream architecture

## Output Files

- `sample_data.csv` - Input CSV file (auto-generated)
- `output_data.json` - Enriched data in JSON format
- `output_data.csv` - Enriched data in CSV format

## Try This

1. Add your own validation rules
2. Create custom enrichment functions
3. Add new output formats (XML, etc.)
4. Process larger CSV files
5. Add more statistical calculations
